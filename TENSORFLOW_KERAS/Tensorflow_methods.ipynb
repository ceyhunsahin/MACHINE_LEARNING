{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6ea7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "def dataset_fn(ds):\n",
    "  return ds.filter(lambda x: x < 5)\n",
    "dataset = dataset.apply(dataset_fn)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6837bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "  print(element)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f401d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e95cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "print(list(dataset.as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5160d12",
   "metadata": {},
   "source": [
    "`as_numpy_iterator()` will preserve the nested structure of dataset elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d3da12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
    "                                              'b': [5, 6]})\n",
    "list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
    "                                      {'a': (2, 4), 'b': 6}]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e347f53",
   "metadata": {},
   "source": [
    "batch(\n",
    "    batch_size, drop_remainder=False, num_parallel_calls=None, deterministic=None,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a858d228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5246d209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3, drop_remainder=True)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56cec5e0",
   "metadata": {},
   "source": [
    "bucket_by_sequence_length(\n",
    "    element_length_func, bucket_boundaries, bucket_batch_sizes, padded_shapes=None,\n",
    "    padding_values=None, pad_to_bucket_boundary=False, no_padding=False,\n",
    "    drop_remainder=False, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6afab058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [5 6 7 0]]\n",
      "[[ 7  8  9 10 11  0]\n",
      " [13 14 15 16 19 20]]\n",
      "[[ 0  0]\n",
      " [21 22]]\n"
     ]
    }
   ],
   "source": [
    "elements = [\n",
    "  [0], [1, 2, 3, 4], [5, 6, 7],\n",
    "  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, tf.int64, output_shapes=[None])\n",
    "dataset = dataset.bucket_by_sequence_length(\n",
    "        element_length_func=lambda elem: tf.shape(elem)[0],\n",
    "        bucket_boundaries=[3, 5],\n",
    "        bucket_batch_sizes=[2, 2, 2])\n",
    "for elem in dataset.as_numpy_iterator():\n",
    "  print(elem)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f07c0da0",
   "metadata": {},
   "source": [
    "cache(\n",
    "    filename='', name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57d70c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "dataset = dataset.map(lambda x: x**2)\n",
    "dataset = dataset.cache()\n",
    "# The first time reading through the data will generate the data using\n",
    "# `range` and `map`.\n",
    "list(dataset.as_numpy_iterator())\n",
    "\n",
    "# Subsequent iterations read from the cache.\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d334e818",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "dataset = dataset.cache(\"/path/to/file\")\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [0, 1, 2, 3, 4]\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.cache(\"/path/to/file\")  # Same file!\n",
    "list(dataset.as_numpy_iterator())\n",
    "# [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5ae48f4",
   "metadata": {},
   "source": [
    "cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd12cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(43)\n",
    "print(dataset.cardinality().numpy())\n",
    "\n",
    "dataset = dataset.repeat()\n",
    "cardinality = dataset.cardinality()\n",
    "print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\n",
    "\n",
    "dataset = dataset.filter(lambda x: True)\n",
    "cardinality = dataset.cardinality()\n",
    "print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eeda35ed",
   "metadata": {},
   "source": [
    "@staticmethod\n",
    "choose_from_datasets(\n",
    "    datasets, choice_dataset, stop_on_empty_dataset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2237ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\n",
    "            tf.data.Dataset.from_tensors(\"bar\").repeat(),\n",
    "            tf.data.Dataset.from_tensors(\"baz\").repeat()]\n",
    "\n",
    "# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\n",
    "choice_dataset = tf.data.Dataset.range(3).repeat(2)\n",
    "\n",
    "result = tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdf5341c",
   "metadata": {},
   "source": [
    "\"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "efa72a1a",
   "metadata": {},
   "source": [
    "concatenate(\n",
    "    dataset, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f95f7f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = a.concatenate(b)\n",
    "print(list(ds.as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee48344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ZipDataset shapes: ((), ()), types: (tf.int64, tf.int64)>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Incompatible types of input datasets: <dtype: 'int64'> vs. (tf.int64, tf.int64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-735b59fd5c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset, name)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \"\"\"\n\u001b[0;32m-> 1246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[1;32m   4843\u001b[0m     \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_legacy_output_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mget_legacy_output_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_to_concatenate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4845\u001b[0;31m       raise TypeError(f\"Incompatible types of input datasets: {output_types} \"\n\u001b[0m\u001b[1;32m   4846\u001b[0m                       f\"vs. {get_legacy_output_types(dataset_to_concatenate)}.\")\n\u001b[1;32m   4847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Incompatible types of input datasets: <dtype: 'int64'> vs. (tf.int64, tf.int64)."
     ]
    }
   ],
   "source": [
    "# The input dataset and dataset to be concatenated should have\n",
    "# compatible element specs.\n",
    "c = tf.data.Dataset.zip((a, b))\n",
    "print(c)\n",
    "print(a.concatenate(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a339a3e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Incompatible types of input datasets: <dtype: 'int64'> vs. <dtype: 'string'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-465fa18d6119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset, name)\u001b[0m\n\u001b[1;32m   1244\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \"\"\"\n\u001b[0;32m-> 1246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate, name)\u001b[0m\n\u001b[1;32m   4843\u001b[0m     \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_legacy_output_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mget_legacy_output_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_to_concatenate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4845\u001b[0;31m       raise TypeError(f\"Incompatible types of input datasets: {output_types} \"\n\u001b[0m\u001b[1;32m   4846\u001b[0m                       f\"vs. {get_legacy_output_types(dataset_to_concatenate)}.\")\n\u001b[1;32m   4847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Incompatible types of input datasets: <dtype: 'int64'> vs. <dtype: 'string'>."
     ]
    }
   ],
   "source": [
    "d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
    "print(a.concatenate(d))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f154fb30",
   "metadata": {},
   "source": [
    "enumerate(\n",
    "    start=0, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3122799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(6, 2)\n",
      "(7, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.enumerate(start=5)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de0c2c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([7, 8], dtype=int32))\n",
      "(1, array([ 9, 10], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "# The (nested) structure of the input dataset determines the\n",
    "# structure of elements in the resulting dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
    "dataset = dataset.enumerate()\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cffa2ef0",
   "metadata": {},
   "source": [
    "filter(\n",
    "    predicate, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff19bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.filter(lambda x: x < 3)\n",
    "x = list(dataset.as_numpy_iterator())\n",
    "print(x)\n",
    "# `tf.math.equal(x, y)` is required for equality comparison\n",
    "def filter_fn(x):\n",
    "  return tf.math.equal(x, 1) # x = 1 olanlari ayiriyor\n",
    "dataset = dataset.filter(filter_fn)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7518db4",
   "metadata": {},
   "source": [
    "flat_map(\n",
    "    map_func, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93c86d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "dataset = dataset.flat_map(\n",
    "    lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7855ca1f",
   "metadata": {},
   "source": [
    "@staticmethod\n",
    "from_generator(\n",
    "    generator, output_types=None, output_shapes=None, args=None,\n",
    "    output_signature=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b127be96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
       "  <tf.RaggedTensor [[1, 2], [3]]>)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen():\n",
    "  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n",
    "  yield 42, ragged_tensor\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "     gen,\n",
    "     output_signature=(\n",
    "         tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n",
    "\n",
    "list(dataset.take(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c951ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 3], dtype=int32), b'A')\n",
      "(array([2, 1], dtype=int32), b'B')\n",
      "(array([3, 3], dtype=int32), b'A')\n",
      "(array([[1, 3],\n",
      "       [2, 3]], dtype=int32), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]], dtype=int32), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]], dtype=int32), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "# Two tensors can be combined into one Dataset object.\n",
    "features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
    "labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "# Both the features and the labels tensors can be converted\n",
    "# to a Dataset object separately and combined after.\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
    "dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n",
    "# A batched feature and label set can be converted to a Dataset\n",
    "# in similar fashion.\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)\n",
    "batched_features = tf.constant([[[1, 3], [2, 3]],\n",
    "                                [[2, 1], [1, 2]],\n",
    "                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3, 2, 1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((batched_features, batched_labels))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccea6406",
   "metadata": {},
   "source": [
    "@staticmethod\n",
    "from_tensors(\n",
    "    tensors, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3ad4a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3], dtype=int32)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9b36958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1, 2, 3], dtype=int32), b'A')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
    "a = list(dataset.as_numpy_iterator())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "550ef592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use `from_tensors` to produce a dataset which repeats\n",
    "# the same example many times.\n",
    "example = tf.constant([1,2,3])\n",
    "dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "461c99d7",
   "metadata": {},
   "source": [
    "get_single_element(\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "776f696a",
   "metadata": {},
   "source": [
    "\n",
    "model = ... # A pre-built or custom model\n",
    "\n",
    "class PreprocessingModel(tf.keras.Model):\n",
    "  def __init__(self, model):\n",
    "    super().__init__(self)\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(input_signature=[...])\n",
    "  def serving_fn(self, data):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n",
    "    ds = ds.batch(batch_size=BATCH_SIZE)\n",
    "    return tf.argmax(self.model(ds.get_single_element()), axis=-1)\n",
    "\n",
    "preprocessing_model = PreprocessingModel(model)\n",
    "your_exported_model_dir = ... # save the model to this path.\n",
    "tf.saved_model.save(preprocessing_model, your_exported_model_dir,\n",
    "              signatures={'serving_default': preprocessing_model.serving_fn}\n",
    "              )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59497828",
   "metadata": {},
   "source": [
    "group_by_window(\n",
    "    key_func, reduce_func, window_size=None, window_size_func=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eac929b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 6 8]\n",
      "[1 3 5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "window_size = 5\n",
    "key_func = lambda x: x%2\n",
    "reduce_func = lambda key, dataset: dataset.batch(window_size)\n",
    "dataset = dataset.group_by_window(\n",
    "          key_func=key_func,\n",
    "          reduce_func=reduce_func,\n",
    "          window_size=window_size)\n",
    "for elem in dataset.as_numpy_iterator():\n",
    "  print(elem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e490c07c",
   "metadata": {},
   "source": [
    "map(\n",
    "    map_func, num_parallel_calls=None, deterministic=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bd73e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "dataset = dataset.map(lambda x: x + 1)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09e3bfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (), types: tf.int64>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "# `map_func` takes a single argument of type `tf.Tensor` with the same\n",
    "# shape and dtype.\n",
    "result = dataset.map(lambda x: x + 1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3387ce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each element is a tuple containing two `tf.Tensor` objects.\n",
    "elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, (tf.int32, tf.string))\n",
    "# `map_func` takes two arguments of type `tf.Tensor`. This function\n",
    "# projects out just the first component.\n",
    "result = dataset.map(lambda x_int, y_str: x_int)\n",
    "list(result.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6452d7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(3,), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "# `map_func` returns two `tf.Tensor` objects.\n",
    "def g(x):\n",
    "  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
    "result = dataset.map(g)\n",
    "result.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c1fd1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python primitives, lists, and NumPy arrays are implicitly converted to\n",
    "# `tf.Tensor`.\n",
    "def h(x):\n",
    "  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
    "result = dataset.map(h)\n",
    "result.element_spec\n",
    "\n",
    "# `map_func` can return nested structures.\n",
    "def i(x):\n",
    "  return (37.0, [42, 16]), \"foo\"\n",
    "result = dataset.map(i)\n",
    "result.element_spec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b81bd2aa",
   "metadata": {},
   "source": [
    "padded_batch(\n",
    "    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6a01e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2 2]\n",
      "[3 3 3]\n",
      "[4 4 4 4]\n",
      "A None\n",
      "----------------------------\n",
      "[[1 0]\n",
      " [2 2]]\n",
      "[[3 3 3 0]\n",
      " [4 4 4 4]]\n",
      "----------------------------\n",
      "[[1 0 0 0 0]\n",
      " [2 2 0 0 0]]\n",
      "[[3 3 3 0 0]\n",
      " [4 4 4 4 0]]\n",
      "----------------------------\n",
      "[[ 1 -1 -1 -1 -1]\n",
      " [ 2  2 -1 -1 -1]]\n",
      "[[ 3  3  3 -1 -1]\n",
      " [ 4  4  4  4 -1]]\n",
      "----------------------------\n",
      "(array([[ 1, -1],\n",
      "       [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
      "       [ 2,  2]], dtype=int32))\n",
      "(array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "A = (tf.data.Dataset\n",
    "     .range(1, 5, output_type=tf.int32)\n",
    "     .map(lambda x: tf.fill([x], x)))\n",
    "def c():\n",
    "    for element in A.as_numpy_iterator():\n",
    "        print(element)\n",
    "print('A', c())\n",
    "\n",
    "print('----------------------------')\n",
    "# Pad to the smallest per-batch size that fits all elements.\n",
    "B = A.padded_batch(2)\n",
    "for element in B.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "# Pad to a fixed size.\n",
    "C = A.padded_batch(2, padded_shapes=5)\n",
    "for element in C.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "# Pad with a custom value.\n",
    "D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
    "for element in D.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "\n",
    "# Components of nested elements can be padded independently.\n",
    "elements = [([1, 2, 3], [10]),\n",
    "            ([4, 5], [11, 12])]\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: iter(elements), (tf.int32, tf.int32))\n",
    "# Pad the first component of the tuple to length 4, and the second\n",
    "# component to the smallest size that fits.\n",
    "dataset = dataset.padded_batch(2,\n",
    "    padded_shapes=([4], [None]),\n",
    "    padding_values=(-1, 100))\n",
    "list(dataset.as_numpy_iterator())\n",
    "\n",
    "\n",
    "# Pad with a single value and multiple components.\n",
    "E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
    "for element in E.as_numpy_iterator():\n",
    "  print(element)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69100099",
   "metadata": {},
   "source": [
    "@staticmethod\n",
    "random(\n",
    "    seed=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6642290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ds1 = tf.data.Dataset.random(seed=4).take(10)\n",
    "ds2 = tf.data.Dataset.random(seed=4).take(10)\n",
    "print(list(ds1.as_numpy_iterator())==list(ds2.as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d521c196",
   "metadata": {},
   "source": [
    "@staticmethod\n",
    "range(\n",
    "    *args, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "265aade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 3.0]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tf.data.Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e9f307a",
   "metadata": {},
   "source": [
    "reduce(\n",
    "    initial_state, reduce_func, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36391619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c87c195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + 1).numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9155528e",
   "metadata": {},
   "source": [
    "shuffle(\n",
    "    buffer_size, seed=None, reshuffle_each_iteration=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7da6fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat(2)\n",
    "# [1, 0, 2, 1, 2, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2ac8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(3)\n",
    "dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
    "dataset = dataset.repeat(2)\n",
    "# [1, 0, 2, 1, 0, 2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d814ca24",
   "metadata": {},
   "source": [
    "@staticmethod\n",
    "zip(\n",
    "    datasets, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19e4acc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The nested structure of the `datasets` argument determines the\n",
    "# structure of elements in the resulting dataset.\n",
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
    "ds = tf.data.Dataset.zip((a, b))\n",
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d80bdbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (5, 2), (6, 3)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.zip((b, a))\n",
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c149497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, array([7, 8]))\n",
      "(2, 5, array([ 9, 10]))\n",
      "(3, 6, array([11, 12]))\n"
     ]
    }
   ],
   "source": [
    "c = tf.data.Dataset.range(7, 13).batch(2)\n",
    "# ==> [ [7, 8],\n",
    "#       [9, 10],\n",
    "#       [11, 12] ]\n",
    "ds = tf.data.Dataset.zip((a, b, c))\n",
    "for element in ds.as_numpy_iterator():\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c04faaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 13), (2, 14)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of elements in the resulting dataset is the same as\n",
    "# the size of the smallest dataset in `datasets`.\n",
    "d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
    "ds = tf.data.Dataset.zip((a, d))\n",
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "18a0d901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 7], dtype=int32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def f():\n",
    "  return x ** 2 + y\n",
    "x = tf.constant([-2, -3])\n",
    "y = tf.Variable([3, -2])\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4df85f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "  ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "  for i in range(len(x)):\n",
    "    ta = ta.write(i, x[i] + 1)\n",
    "  return ta.stack()\n",
    "f(tf.constant([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "22871356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\n",
       "array([[[1., 0., 0.],\n",
       "        [0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [[0, 2], [1, -1]]\n",
    "depth = 3\n",
    "tf.one_hot(indices, depth,\n",
    "           on_value=1.0, off_value=0.0,\n",
    "           axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a697f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa188953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9ca59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17616c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc626d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0eb227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a7b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
