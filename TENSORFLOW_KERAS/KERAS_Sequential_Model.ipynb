{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035de178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6c10e",
   "metadata": {},
   "source": [
    "A `Sequential model` is appropriate for a **plain stack of layers** where each layer has **exactly one input tensor and one output tensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6734b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    layers.Dense(4, activation = 'relu', name = 'layer1'),\n",
    "    layers.Dense(5, activation = 'relu', name = 'layer2'),\n",
    "    layers.Dense(7, name = 'layer3')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c6c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((3, 5))\n",
    "y = model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0711d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (3, 4)                    24        \n",
      "                                                                 \n",
      " layer2 (Dense)              (3, 5)                    25        \n",
      "                                                                 \n",
      " layer3 (Dense)              (3, 7)                    42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91\n",
      "Trainable params: 91\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d3a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 layers\n",
    "layer1 = layers.Dense(2, activation=\"relu\", name=\"layer1\")\n",
    "layer2 = layers.Dense(3, activation=\"relu\", name=\"layer2\")\n",
    "layer3 = layers.Dense(4, name=\"layer3\")\n",
    "\n",
    "# Call layers on a test input\n",
    "x = tf.ones((3, 3))\n",
    "y = layer3(layer2(layer1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d51df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\"))\n",
    "model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ba34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"my_sequential\")\n",
    "model.add(layers.Dense(2, activation=\"relu\", name=\"layer1\"))\n",
    "model.add(layers.Dense(3, activation=\"relu\", name=\"layer2\"))\n",
    "model.add(layers.Dense(4, name=\"layer3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11859bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.ones((3,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a427598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "938df1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 0.27054474,  0.7589039 , -1.2569561 , -0.7810746 ],\n",
       "       [ 0.27054474,  0.7589039 , -1.2569561 , -0.7810746 ],\n",
       "       [ 0.27054474,  0.7589039 , -1.2569561 , -0.7810746 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f395c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layers.Dense(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a698abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weights # Generally, all layers in Keras need to know the shape of their \n",
    "#inputs in order to be able to create their weights. \n",
    "#So when you create a layer like this, initially, it has no weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ec70134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[ 0.11869502, -0.28982425,  0.40335488],\n",
       "        [ 0.37651706, -0.75388896, -0.29197264],\n",
       "        [-0.27107036, -0.53404474, -0.4815399 ],\n",
       "        [-0.15520489, -0.58162254, -0.00474489]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call layer on a test input\n",
    "x = tf.ones((1, 4))\n",
    "y = layer(x)\n",
    "layer.weights  # Now it has weights, of shape (4, 3) and (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356d9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights after calling the model: 6\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation=\"relu\"),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(4),\n",
    "    ]\n",
    ")  # No weights at this stage!\n",
    "\n",
    "# At this point, you can't do this:\n",
    "# model.weights\n",
    "\n",
    "# You also can't do this:\n",
    "# model.summary()\n",
    "\n",
    "# Call the model on a test input\n",
    "x = tf.ones((1, 4))\n",
    "y = model(x)\n",
    "print(\"Number of weights after calling the model:\", len(model.weights))  # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c7a2692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (1, 2)                    10        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (1, 3)                    9         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (1, 4)                    16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35\n",
      "Trainable params: 35\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # to display its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "454f077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 123, 123, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 121, 121, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 40, 40, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,680\n",
      "Trainable params: 11,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 123, 123, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 121, 121, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 40, 40, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 38, 38, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 36, 36, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 12, 12, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 10, 10, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,672\n",
      "Trainable params: 48,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(250, 250, 3)))  # 250x250 RGB images\n",
    "model.add(layers.Conv2D(32, 5, strides=2, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "\n",
    "# Can you guess what the current output shape is at this point? Probably not.\n",
    "# Let's just print it:\n",
    "print(model.summary())\n",
    "\n",
    "# The answer was: (40, 40, 32), so we can keep downsampling...\n",
    "\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "# And now?\n",
    "model.summary()\n",
    "\n",
    "# Now that we have 4x4 feature maps, time to apply global max pooling.\n",
    "model.add(layers.GlobalMaxPooling2D())\n",
    "\n",
    "# Finally, we add a classification layer.\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea5e38bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "# With `dilation_rate` as 2.\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "2, 5, activation='relu', dilation_rate=3,input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n",
    "#(5-1)*Dilation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d585a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11, 11, 2)\n"
     ]
    }
   ],
   "source": [
    "# With `dilation_rate` as 2.\n",
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(\n",
    "2, 7, activation='relu',  strides = 2,input_shape=input_shape[1:])(x)\n",
    "print(y.shape)\n",
    "# 28-5 + 1, strides ile yarisi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e040b2d",
   "metadata": {},
   "source": [
    "## Feature extraction with a Sequential model\n",
    "Once a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "680e9738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 123, 123, 32), dtype=float32, numpy=\n",
       " array([[[[0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          ...,\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565]],\n",
       " \n",
       "         [[0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          ...,\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565]],\n",
       " \n",
       "         [[0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          ...,\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          ...,\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565]],\n",
       " \n",
       "         [[0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          ...,\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565]],\n",
       " \n",
       "         [[0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          [0.        , 0.78135425, 0.35112116, ..., 0.12335476,\n",
       "           0.48590237, 0.00471565],\n",
       "          ...,\n",
       "          [0.        , 0.78135437, 0.35112116, ..., 0.12335466,\n",
       "           0.4859023 , 0.00471565],\n",
       "          [0.        , 0.78135437, 0.35112116, ..., 0.12335466,\n",
       "           0.4859023 , 0.00471565],\n",
       "          [0.        , 0.78135437, 0.35112116, ..., 0.12335466,\n",
       "           0.4859023 , 0.00471565]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 121, 121, 32), dtype=float32, numpy=\n",
       " array([[[[0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619]],\n",
       " \n",
       "         [[0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619]],\n",
       " \n",
       "         [[0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619]],\n",
       " \n",
       "         [[0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619]],\n",
       " \n",
       "         [[0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732005,\n",
       "           0.52582926, 0.18936619],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.56991065, ..., 0.25732008,\n",
       "           0.52582926, 0.18936619],\n",
       "          [0.        , 0.        , 0.5699106 , ..., 0.25732005,\n",
       "           0.5258293 , 0.18936618],\n",
       "          [0.        , 0.        , 0.5699107 , ..., 0.25732005,\n",
       "           0.52582926, 0.18936619]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 119, 119, 32), dtype=float32, numpy=\n",
       " array([[[[0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          ...,\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065]],\n",
       " \n",
       "         [[0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          ...,\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065]],\n",
       " \n",
       "         [[0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          ...,\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          ...,\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065]],\n",
       " \n",
       "         [[0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          ...,\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065]],\n",
       " \n",
       "         [[0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904106, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          ...,\n",
       "          [0.15904105, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.15904108, 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399065],\n",
       "          [0.1590411 , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.06399063]]]], dtype=float32)>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(250, 250, 3)),\n",
    "        layers.Conv2D(32, 5, strides=2, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=initial_model.inputs,\n",
    "    outputs=[layer.output for layer in initial_model.layers],\n",
    ")\n",
    "\n",
    "# Call feature extractor on test input.\n",
    "x = tf.ones((1, 250, 250, 3))\n",
    "features = feature_extractor(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef0a00d",
   "metadata": {},
   "source": [
    "## Transfer learning with a Sequential model\n",
    "Transfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our guide to transfer learning.\n",
    "\n",
    "Here are two common transfer learning blueprint involving Sequential models.\n",
    "\n",
    "First, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model.layers and set layer.trainable = False on each layer, except the last one. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ad350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(784)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "\n",
    "# Presumably you would want to first load pre-trained weights.\n",
    "model.load_weights(...)\n",
    "\n",
    "# Freeze all layers except the last one.\n",
    "for layer in model.layers[:-1]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Recompile and train (this will only update the weights of the last layer).\n",
    "model.compile(...)\n",
    "model.fit(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
